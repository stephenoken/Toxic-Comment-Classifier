{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classifier DNN \n",
    "\n",
    "This notebook will focus on the use of Deep Neural Networks to tackle the problem of tox comment classification. Starting from the work done in the `toxic-comment-classifier-classical-model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('./data/train.csv')\n",
    "\n",
    "X = data['comment_text']\n",
    "y = data[data.columns[2:]]\n",
    "\n",
    "# processed_data_set = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# X_train_raw, X_test_raw, y_train, y_test = processed_data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we've started off loading and splitting our data into test/training datasets. We've appended `raw` to the `X_[train|test]` because the comments will be need to be processed and converted to vectors before being feed into a nueural network. \n",
    "\n",
    "## Prepraring the data\n",
    "\n",
    "We'll need to convert our comments to vectors. We're going to do this by assigning an unique ID to each word in our corpus. We'll then convert our comments into vectors  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind \n",
    "import string \n",
    "import re \n",
    "import numpy as np \n",
    "from collections import Counter \n",
    "from functools import reduce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Word Occurances\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 04:10:51\n",
      "Map comments to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    explanation\\nwhy the edits made under my usern...\n",
      "0    d ' aww !  he matches this background colour i...\n",
      "0    hey man ,  i ' m really not trying to edit war...\n",
      "0     \" \\nmore\\ni can ' t make any real suggestions...\n",
      "0    you ,  sir ,  are my hero .  any chance you re...\n",
      "0     \" \\n\\ncongratulations from me as well ,  use ...\n",
      "0         cocksucker before you piss around on my work\n",
      "0    your vandalism to the matt shirvington article...\n",
      "0    sorry if the word  ' nonsense '  was offensive...\n",
      "0    alignment on this subject and which are contra...\n",
      "0     \" \\nfair use rationale for image : wonju . jp...\n",
      "0    bbq \\n\\nbe a man and lets discuss it - maybe o...\n",
      "0    hey .  .  .  what is it .  . \\n @   |  talk  ....\n",
      "0    before you start throwing accusations and warn...\n",
      "0    oh ,  and the girl above started her arguments...\n",
      "0     \" \\n\\njuelz santanas age\\n\\nin 2002 ,  juelz ...\n",
      "0    bye !  \\n\\ndon ' t look ,  come or think of co...\n",
      "0    redirect talk : voydan pop georgiev -  chernod...\n",
      "0    the mitsurugi point made no sense  -  why not ...\n",
      "0    don ' t mean to bother you \\n\\ni see that you ...\n",
      "0     \" \\n\\n regarding your recent edits \\n\\nonce a...\n",
      "0     \" \\ngood to know .  about me ,  yeah ,  i ' m...\n",
      "0     \" \\n\\n snowflakes are not always symmetrical ...\n",
      "0     \" \\n\\n the signpost :  24 september 2012 \\n\\n...\n",
      "0     \" \\n\\nre - considering 1st paragraph edit ? \\...\n",
      "0    radial symmetry \\n\\nseveral now extinct lineag...\n",
      "0    there ' s no need to apologize .  a wikipedia ...\n",
      "0    yes ,  because the mother of the child in the ...\n",
      "0     \" \\nok .  but it will take a bit of work but ...\n",
      "0     \"  =  =  a barnstar for you !   =  = \\n\\n  th...\n",
      "                           ...                        \n",
      "0    your absurd edits \\n\\nyour absurd edits on gre...\n",
      "0    maybe he ' s got better things to do than spen...\n",
      "0    scrap that ,  it does meet criteria and its go...\n",
      "0                                you could do worse . \n",
      "0     ,  7 march 2011  ( utc ) \\nare you also user ...\n",
      "0     \" \\n\\nhey listen don ' t you ever !  !  !  ! ...\n",
      "0                   thank you very ,  very much .   ·✆\n",
      "0                        talkback :  15 september 2012\n",
      "0                   2005  ( utc ) \\n 06 : 35 ,  31 mar\n",
      "0    i agree /  on another note lil wayne is a tale...\n",
      "0    while about half the references are from byu -...\n",
      "0    prague spring \\n\\ni think that prague spring d...\n",
      "0    i see this as having been merged ;  undoing on...\n",
      "0    and i ' m going to keep posting the stuff u de...\n",
      "0     \" \\n\\nhow come when you download that mp3 it ...\n",
      "0    i ' ll be on irc ,  too ,  if you have a more ...\n",
      "0    it is my opinion that that happens to be off -...\n",
      "0    please stop removing content from wikipedia ; ...\n",
      "0    image : barack - obama - mother . jpg listed f...\n",
      "0     \" editing of article without consensus  &  re...\n",
      "0     \" \\nno he did not ,  read it again  ( i would...\n",
      "0     \" \\n auto guides and the motoring press are n...\n",
      "0     \" \\nplease identify what part of blp applies ...\n",
      "0    catalan independentism is the social movement ...\n",
      "0    the numbers in parentheses are the additional ...\n",
      "0     \"  :  :  :  :  : and for the second time of a...\n",
      "0    you should be ashamed of yourself \\n\\nthat is ...\n",
      "0    spitzer \\n\\numm ,  theres no actual article fo...\n",
      "0    and it looks like it was actually you who put ...\n",
      "0     \" \\nand  .  .  .  i really don ' t think you ...\n",
      "Length: 159571, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [706, 91, 2, 145, 148, 198, 42, 693, 4507, 114...\n",
      "1    [184, 9, 16468, 16, 64, 2654, 19, 576, 3810, 6...\n",
      "2    [434, 445, 3, 6, 9, 83, 152, 21, 276, 5, 90, 3...\n",
      "3    [4, 72, 6, 48, 9, 32, 114, 69, 352, 1455, 23, ...\n",
      "4    [10, 3, 1696, 3, 28, 42, 3463, 1, 69, 1087, 10...\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:05\n"
     ]
    }
   ],
   "source": [
    "# counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(X), title='Counting Word Occurances')\n",
    "\n",
    "def concat_counts(counter_series, comment):\n",
    "    counter_res, series_res = counter_series\n",
    "    text = ''.join([c if c not in string.punctuation else ' ' + c + ' ' for c in comment]).lower()\n",
    "    pbar.update()\n",
    "    counter = Counter()\n",
    "    counter.update(text.split())\n",
    "    return(counter_res + counter, series_res.append(pd.Series(text)))\n",
    "\n",
    "# for i, comment in enumerate(X[:10]):\n",
    "#     text = ''.join([c if c not in string.punctuation else ' ' + c + ' ' for c in comment]).lower()\n",
    "#     pbar.update()\n",
    "\n",
    "counts, X_encoded = reduce(concat_counts, X, (Counter(), pd.Series())) \n",
    "\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "word_2_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "pbar = pyprind.ProgBar(len(X), title='Map comments to ints')\n",
    "\n",
    "def map_comments(comment):\n",
    "    mapped_comments = [word_2_int[word] for word in comment.split()]\n",
    "    pbar.update()\n",
    "    return mapped_comments\n",
    "    \n",
    "x_mapped = pd.Series(map(map_comments, X_encoded))\n",
    "\n",
    "print(x_mapped.iloc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump(x_mapped, open('./pickles/mapped-comments.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to reduce the sequence length for all of our comments. This will be our first hyper parameter that we can tweak. For simplicity we'll hardcode our sequence length to 75 words. 75 was chosen because it was the value for the 75th percentile of the `comment_word_count` in the temp_data dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor this as part of the predict and fit methods\n",
    "sequence_length = 75\n",
    "sequences = np.zeros((len(x_mapped), sequence_length), dtype=int)\n",
    "\n",
    "for i, row in enumerate(x_mapped):\n",
    "    comments_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = comments_arr[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =  train_test_split(sequences, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object create_batch_generator at 0x114060bf8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x)\n",
    "    x = x[: n_batches * batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches * batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii: ii + batch_size], y[ii: ii + batch_size]\n",
    "        else:\n",
    "            yield x[ii: ii + batch_size]\n",
    "                \n",
    "xs = create_batch_generator(X_train)\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# n_words = max(list(word_2_int.values())) + 1\n",
    "# embedding = tf.Variable(tf.random_uniform(shape=(n_words, 256), minval=-1, maxval=1))\n",
    "\n",
    "# embed_x = tf.nn.embedding_lookup(embedding, )\n",
    "\n",
    "class ToxicRNN(object):\n",
    "    def __init__(self, n_words, seq_len=75, lstm_size=256, num_layers=1, batch_size=64, learning_rate=.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            self.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self):\n",
    "        ## Define the placholders \n",
    "        tf_x = tf.placeholder(tf.int32, shape=(self.batch_size, self.seq_len),\n",
    "                             name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32, shape=(self.batch_size), name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32, name='tf_keepprob')\n",
    "        \n",
    "        ## Define LSTM cell and stack them together\n",
    "        embedding = tf.Variable(tf.random_uniform(shape=(self.n_words, self.embed_size), minval=-1, maxval=1),\n",
    "                               name=\"embedding\")\n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name=\"embedded_x\")\n",
    "        \n",
    "        ## Define LSTM cell and stack them together \n",
    "        cells = tf.contrib.rnn.MultiRNNCell([\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "            tf.contrib.rnn.BasicLSTMCell(self.lstm_size), output_keep_prob=tf_keepprob)\n",
    "            for i in range(self.num_layers)\n",
    "        ])\n",
    "        \n",
    "        ## Define the initial state\n",
    "        self.init_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print(' << initial state >> ', self.inital_state)\n",
    "        \n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, embed_x, initial_state=self.initial_state)\n",
    "        \n",
    "        ## Note: lstm_outputs shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
