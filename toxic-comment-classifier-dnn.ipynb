{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classifier DNN \n",
    "\n",
    "This notebook will focus on the use of Deep Neural Networks to tackle the problem of tox comment classification. Starting from the work done in the `toxic-comment-classifier-classical-model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155000, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('./data/train.csv')[:155000]\n",
    "print(data.shape)\n",
    "X = data['comment_text']\n",
    "y = data[data.columns[2:]]\n",
    "\n",
    "# processed_data_set = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# X_train_raw, X_test_raw, y_train, y_test = processed_data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we've started off loading and splitting our data into test/training datasets. We've appended `raw` to the `X_[train|test]` because the comments will be need to be processed and converted to vectors before being feed into a nueural network. \n",
    "\n",
    "## Prepraring the data\n",
    "\n",
    "We'll need to convert our comments to vectors. We're going to do this by assigning an unique ID to each word in our corpus. We'll then convert our comments into vectors  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind \n",
    "import string \n",
    "import re \n",
    "import numpy as np \n",
    "from collections import Counter \n",
    "from functools import reduce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = Counter()\n",
    "if False:\n",
    "    pbar = pyprind.ProgBar(len(X), title='Counting Word Occurances')\n",
    "\n",
    "    def concat_counts(counter_series, comment):\n",
    "        counter_res, series_res = counter_series\n",
    "        text = ''.join([c if c not in string.punctuation else ' ' + c + ' ' for c in comment]).lower()\n",
    "        pbar.update()\n",
    "        counter = Counter()\n",
    "        counter.update(text.split())\n",
    "        return(counter_res + counter, series_res.append(pd.Series(text)))\n",
    "\n",
    "    # for i, comment in enumerate(X[:10]):\n",
    "    #     text = ''.join([c if c not in string.punctuation else ' ' + c + ' ' for c in comment]).lower()\n",
    "    #     pbar.update()\n",
    "\n",
    "    counts, X_encoded = reduce(concat_counts, X, (Counter(), pd.Series())) \n",
    "\n",
    "    word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "    word_2_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "    pbar = pyprind.ProgBar(len(X), title='Map comments to ints')\n",
    "\n",
    "    def map_comments(comment):\n",
    "        mapped_comments = [word_2_int[word] for word in comment.split()]\n",
    "        pbar.update()\n",
    "        return mapped_comments\n",
    "\n",
    "    x_mapped = pd.Series(map(map_comments, X_encoded))\n",
    "\n",
    "    print(x_mapped.iloc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# pickle.dump(x_mapped, open('./pickles/mapped-comments.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4585\n",
      "0    [711, 91, 2, 145, 148, 199, 42, 685, 4487, 116...\n",
      "1    [183, 9, 16438, 16, 63, 2646, 19, 575, 3785, 6...\n",
      "2    [434, 445, 3, 6, 9, 83, 152, 21, 276, 5, 90, 3...\n",
      "3    [4, 72, 6, 48, 9, 32, 114, 69, 353, 1451, 23, ...\n",
      "4    [10, 3, 1713, 3, 28, 42, 3493, 1, 69, 1093, 10...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from bounter import bounter \n",
    "\n",
    "def process_comment(comment):\n",
    "    characters = [c.lower() if c not in string.punctuation else ' ' + c + ' ' for c in comment]\n",
    "    return ''.join(characters).split()\n",
    "\n",
    "counts = bounter(size_mb=4096)\n",
    "X_processed = X.apply(process_comment)\n",
    "X_processed.apply(lambda x: counts.update(x))\n",
    "print(counts['sorry'])\n",
    "\n",
    "word_counts = sorted(counts, key=lambda x: counts[x], reverse=True)\n",
    "\n",
    "word_2_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "def map_comments(comment):\n",
    "    mapped_comments = [word_2_int[word] for word in comment]\n",
    "    return mapped_comments\n",
    "    \n",
    "X_encoded = pd.Series(map(map_comments, X_processed))\n",
    "\n",
    "print(X_encoded.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to reduce the sequence length for all of our comments. This will be our first hyper parameter that we can tweak. For simplicity we'll hardcode our sequence length to 75 words. 75 was chosen because it was the value for the 75th percentile of the `comment_word_count` in the temp_data dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor this as part of the predict and fit methods\n",
    "sequence_length = 75\n",
    "sequences = np.zeros((len(X_encoded), sequence_length), dtype=int)\n",
    "\n",
    "for i, row in enumerate(X_encoded):\n",
    "    comments_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = comments_arr[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124000, 75) (124000, 6)\n",
      "(31000, 75) (31000, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test =  train_test_split(sequences, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x)\n",
    "    x = x[: n_batches * batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches * batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii: ii + batch_size], y[ii: ii + batch_size]\n",
    "        else:\n",
    "            yield x[ii: ii + batch_size]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# n_words = max(list(word_2_int.values())) + 1\n",
    "# embedding = tf.Variable(tf.random_uniform(shape=(n_words, 256), minval=-1, maxval=1))\n",
    "\n",
    "# embed_x = tf.nn.embedding_lookup(embedding, )\n",
    "\n",
    "class ToxicRNN(object):\n",
    "    def __init__(self, n_words, seq_len=75, lstm_size=256, num_layers=1, batch_size=64, learning_rate=.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self):\n",
    "        ## Define the placholders \n",
    "        tf_x = tf.placeholder(tf.int32, shape=(self.batch_size, self.seq_len),\n",
    "                             name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32, shape=(self.batch_size), name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32, name='tf_keepprob')\n",
    "        \n",
    "        ## Define LSTM cell and stack them together\n",
    "        embedding = tf.Variable(tf.random_uniform(shape=(self.n_words, self.embed_size), minval=-1, maxval=1),\n",
    "                               name=\"embedding\")\n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name=\"embedded_x\")\n",
    "        \n",
    "        ## Define LSTM cell and stack them together \n",
    "        cells = tf.contrib.rnn.MultiRNNCell([\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "            tf.contrib.rnn.BasicLSTMCell(self.lstm_size), output_keep_prob=tf_keepprob)\n",
    "            for i in range(self.num_layers)\n",
    "        ])\n",
    "        \n",
    "        ## Define the initial state\n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print(' << initial state >> ', self.initial_state)\n",
    "        \n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, embed_x, initial_state=self.initial_state)\n",
    "        \n",
    "        ## Note: lstm_outputs shape:\n",
    "        ## [batch_size, max_time, cells.output_size]\n",
    "        print('\\n << lst_output >>', lstm_outputs)\n",
    "        print('\\n << final state >>', self.final_state)\n",
    "        \n",
    "        logits = tf.layers.dense(\n",
    "            inputs=lstm_outputs[:, -1],\n",
    "            units=1, activation=None,\n",
    "            name='logits')\n",
    "        \n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print('\\n << logits >>', logits)\n",
    "        \n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        predictions = {\n",
    "            'probabilities': y_proba,\n",
    "            'labels': tf.cast(tf.round(y_proba), tf.int32,\n",
    "                             name='labels')\n",
    "        }\n",
    "        \n",
    "        ## Define the cost function \n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf_y, logits=logits), name='cost'\n",
    "            )\n",
    "        \n",
    "        ## Define the optimiser \n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "        \n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "                \n",
    "                for  batch_x, batch_y in create_batch_generator(\n",
    "                    X_train, y_train, self.batch_size):\n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                           'tf_y:0': batch_y,\n",
    "                           'tf_keepprob:0': .5,\n",
    "                           self.initial_state: state}\n",
    "                    loss, _, state = sess.run(\n",
    "                        ['cost:0', 'train_op', self.final_state], feed_dict=feed\n",
    "                    )\n",
    "                    \n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\"Epoch: %d/%d Iteration: %d \"\n",
    "                              \"| Train loss: %.5f\" % (epoch + 1, num_epochs, iteration, loss))\n",
    "                    iteration += 1 \n",
    "                    if(epoch + 1) % 10 == 0:\n",
    "                        self.saver.save(sess, \"model/sentiment-%d.ckpt\" % epoch)\n",
    "            \n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess, tf.train.latest_checkpoint('./model'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for ii, batch_x in enumerate(create_batch_generator(X_data, None, batch_size=self.batch_size), 1):\n",
    "                feed = {'tf_x:0': batch_x,\n",
    "                       'tf_keepprob:0': 1.0,\n",
    "                       self.initial_state: test_state}\n",
    "                if return_proba:\n",
    "                    pred, test_state = sess.run(['probabilities:0', self.final_state],\n",
    "                                               feed_dict=feed)\n",
    "                else:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['labels:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                preds.append(pred)\n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(1000, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(1000, 128) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState_1/BasicLSTMCellZeroState/zeros:0' shape=(1000, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState_1/BasicLSTMCellZeroState/zeros_1:0' shape=(1000, 128) dtype=float32>))\n",
      "\n",
      " << lst_output >> Tensor(\"rnn/transpose_1:0\", shape=(1000, 75, 128), dtype=float32)\n",
      "\n",
      " << final state >> (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(1000, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(1000, 128) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_5:0' shape=(1000, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_6:0' shape=(1000, 128) dtype=float32>))\n",
      "\n",
      " << logits >> Tensor(\"logits_squeezed:0\", shape=(1000,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "n_words = max(list(word_2_int.values())) + 1\n",
    "\n",
    "rnn = ToxicRNN(n_words=n_words, seq_len=sequence_length, embed_size=256, lstm_size=128, \n",
    "               num_layers=2, batch_size=1000, learning_rate=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15 Iteration: 20 | Train loss: 0.32462\n",
      "Epoch: 1/15 Iteration: 40 | Train loss: 0.23937\n",
      "Epoch: 1/15 Iteration: 60 | Train loss: 0.19991\n",
      "Epoch: 1/15 Iteration: 80 | Train loss: 0.19663\n",
      "Epoch: 1/15 Iteration: 100 | Train loss: 0.19579\n",
      "Epoch: 1/15 Iteration: 120 | Train loss: 0.16000\n",
      "Epoch: 2/15 Iteration: 140 | Train loss: 0.14174\n",
      "Epoch: 2/15 Iteration: 160 | Train loss: 0.16750\n",
      "Epoch: 2/15 Iteration: 180 | Train loss: 0.11262\n",
      "Epoch: 2/15 Iteration: 200 | Train loss: 0.13215\n",
      "Epoch: 2/15 Iteration: 220 | Train loss: 0.11474\n",
      "Epoch: 2/15 Iteration: 240 | Train loss: 0.12718\n",
      "Epoch: 3/15 Iteration: 260 | Train loss: 0.10366\n",
      "Epoch: 3/15 Iteration: 280 | Train loss: 0.09421\n",
      "Epoch: 3/15 Iteration: 300 | Train loss: 0.08693\n",
      "Epoch: 3/15 Iteration: 320 | Train loss: 0.08789\n",
      "Epoch: 3/15 Iteration: 340 | Train loss: 0.10389\n",
      "Epoch: 3/15 Iteration: 360 | Train loss: 0.08608\n",
      "Epoch: 4/15 Iteration: 380 | Train loss: 0.08535\n",
      "Epoch: 4/15 Iteration: 400 | Train loss: 0.07277\n",
      "Epoch: 4/15 Iteration: 420 | Train loss: 0.07421\n",
      "Epoch: 4/15 Iteration: 440 | Train loss: 0.07666\n",
      "Epoch: 4/15 Iteration: 460 | Train loss: 0.07019\n",
      "Epoch: 4/15 Iteration: 480 | Train loss: 0.06156\n",
      "Epoch: 5/15 Iteration: 500 | Train loss: 0.05984\n",
      "Epoch: 5/15 Iteration: 520 | Train loss: 0.05761\n",
      "Epoch: 5/15 Iteration: 540 | Train loss: 0.05038\n",
      "Epoch: 5/15 Iteration: 560 | Train loss: 0.06239\n",
      "Epoch: 5/15 Iteration: 580 | Train loss: 0.05338\n",
      "Epoch: 5/15 Iteration: 600 | Train loss: 0.04992\n",
      "Epoch: 5/15 Iteration: 620 | Train loss: 0.05867\n",
      "Epoch: 6/15 Iteration: 640 | Train loss: 0.06465\n",
      "Epoch: 6/15 Iteration: 660 | Train loss: 0.03691\n",
      "Epoch: 6/15 Iteration: 680 | Train loss: 0.03186\n",
      "Epoch: 6/15 Iteration: 700 | Train loss: 0.04028\n",
      "Epoch: 6/15 Iteration: 720 | Train loss: 0.04468\n",
      "Epoch: 6/15 Iteration: 740 | Train loss: 0.03546\n",
      "Epoch: 7/15 Iteration: 760 | Train loss: 0.05112\n",
      "Epoch: 7/15 Iteration: 780 | Train loss: 0.04013\n",
      "Epoch: 7/15 Iteration: 800 | Train loss: 0.03438\n",
      "Epoch: 7/15 Iteration: 820 | Train loss: 0.03014\n",
      "Epoch: 7/15 Iteration: 840 | Train loss: 0.03490\n",
      "Epoch: 7/15 Iteration: 860 | Train loss: 0.02556\n",
      "Epoch: 8/15 Iteration: 880 | Train loss: 0.02448\n",
      "Epoch: 8/15 Iteration: 900 | Train loss: 0.02642\n",
      "Epoch: 8/15 Iteration: 920 | Train loss: 0.02027\n",
      "Epoch: 8/15 Iteration: 940 | Train loss: 0.03019\n",
      "Epoch: 8/15 Iteration: 960 | Train loss: 0.04098\n",
      "Epoch: 8/15 Iteration: 980 | Train loss: 0.04077\n",
      "Epoch: 9/15 Iteration: 1000 | Train loss: 0.04313\n",
      "Epoch: 9/15 Iteration: 1020 | Train loss: 0.02602\n",
      "Epoch: 9/15 Iteration: 1040 | Train loss: 0.02401\n",
      "Epoch: 9/15 Iteration: 1060 | Train loss: 0.05083\n",
      "Epoch: 9/15 Iteration: 1080 | Train loss: 0.03528\n",
      "Epoch: 9/15 Iteration: 1100 | Train loss: 0.02723\n",
      "Epoch: 10/15 Iteration: 1120 | Train loss: 0.01565\n",
      "Epoch: 10/15 Iteration: 1140 | Train loss: 0.01023\n",
      "Epoch: 10/15 Iteration: 1160 | Train loss: 0.00843\n",
      "Epoch: 10/15 Iteration: 1180 | Train loss: 0.02596\n",
      "Epoch: 10/15 Iteration: 1200 | Train loss: 0.02440\n",
      "Epoch: 10/15 Iteration: 1220 | Train loss: 0.01320\n",
      "Epoch: 10/15 Iteration: 1240 | Train loss: 0.01346\n",
      "Epoch: 11/15 Iteration: 1260 | Train loss: 0.01891\n",
      "Epoch: 11/15 Iteration: 1280 | Train loss: 0.00861\n",
      "Epoch: 11/15 Iteration: 1300 | Train loss: 0.00400\n",
      "Epoch: 11/15 Iteration: 1320 | Train loss: 0.02691\n",
      "Epoch: 11/15 Iteration: 1340 | Train loss: 0.01252\n",
      "Epoch: 11/15 Iteration: 1360 | Train loss: 0.00472\n",
      "Epoch: 12/15 Iteration: 1380 | Train loss: 0.00935\n",
      "Epoch: 12/15 Iteration: 1400 | Train loss: 0.01224\n",
      "Epoch: 12/15 Iteration: 1420 | Train loss: 0.00484\n",
      "Epoch: 12/15 Iteration: 1440 | Train loss: 0.02236\n",
      "Epoch: 12/15 Iteration: 1460 | Train loss: 0.01158\n",
      "Epoch: 12/15 Iteration: 1480 | Train loss: 0.00608\n",
      "Epoch: 13/15 Iteration: 1500 | Train loss: 0.01097\n",
      "Epoch: 13/15 Iteration: 1520 | Train loss: 0.00825\n",
      "Epoch: 13/15 Iteration: 1540 | Train loss: 0.00758\n",
      "Epoch: 13/15 Iteration: 1560 | Train loss: 0.01555\n",
      "Epoch: 13/15 Iteration: 1580 | Train loss: 0.00364\n",
      "Epoch: 13/15 Iteration: 1600 | Train loss: 0.00488\n",
      "Epoch: 14/15 Iteration: 1620 | Train loss: 0.00526\n",
      "Epoch: 14/15 Iteration: 1640 | Train loss: 0.00707\n",
      "Epoch: 14/15 Iteration: 1660 | Train loss: 0.00345\n",
      "Epoch: 14/15 Iteration: 1680 | Train loss: 0.01163\n",
      "Epoch: 14/15 Iteration: 1700 | Train loss: 0.01147\n",
      "Epoch: 14/15 Iteration: 1720 | Train loss: 0.00505\n",
      "Epoch: 15/15 Iteration: 1740 | Train loss: 0.00656\n",
      "Epoch: 15/15 Iteration: 1760 | Train loss: 0.00371\n",
      "Epoch: 15/15 Iteration: 1780 | Train loss: 0.00441\n",
      "Epoch: 15/15 Iteration: 1800 | Train loss: 0.00924\n",
      "Epoch: 15/15 Iteration: 1820 | Train loss: 0.00605\n",
      "Epoch: 15/15 Iteration: 1840 | Train loss: 0.00458\n",
      "Epoch: 15/15 Iteration: 1860 | Train loss: 0.01301\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train['toxic'].iloc, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/sentiment-9.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.predict(X_test[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/sentiment-9.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9428916582847495"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rnn.predict(X_test, return_proba=True)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test['toxic'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.9485721920243085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come on . i think you know the truth . flewis you pulled me up on spelling , warnabool which is completely irrelevant and my faith in god . however , you never denied being a nerd / loser . now you can listen to me and change or be in your usual stubborn attitude and just continue your miserable , worthless life .'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_2_word = {ii: word  for ii, word in enumerate(word_counts, 1)}\n",
    "\" \".join([int_2_word[x] for x in X_test[0] if x != 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
