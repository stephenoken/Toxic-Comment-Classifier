{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classifier DNN \n",
    "\n",
    "This notebook will focus on the use of Deep Neural Networks to tackle the problem of tox comment classification. Starting from the work done in the `toxic-comment-classifier-classical-model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('./data/train.csv')\n",
    "print(data.shape)\n",
    "X = data['comment_text'].values\n",
    "y = data[data.columns[2:]].values\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
    "tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grandma Terri Should Burn in Trash \n",
      "Grandma Terri is trash. I hate Grandma Terri. F%%K her to HELL! 71.74.76.40\n",
      "[12927, 8296, 56, 3980, 10, 4414, 12927, 8296, 8, 4414, 7, 398, 12927, 8296, 871, 1369, 184, 2, 866, 1697, 2609, 1738, 1336]\n",
      "[12927  8296    56  3980    10  4414 12927  8296     8  4414     7   398\n",
      " 12927  8296   871  1369   184     2   866  1697  2609  1738  1336     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 70\n",
    "\n",
    "padded_train = pad_sequences(tokenized_train, maxlen=max_len, padding='post')\n",
    "padded_test = pad_sequences(tokenized_test, maxlen=max_len, padding='post')\n",
    "print(X_train[0])\n",
    "print(tokenized_train[0])\n",
    "print(padded_train[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a pretrained Word2Vector as the basis of our embedded vocabulary. The pretrained Word2Vec model is going to be Facebook's [fastText](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec). For more info please find it [here](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_dim = 300\n",
    "\n",
    "def process_pretrained_word_vec(line):\n",
    "    values = line.rstrip().rsplit(' ', embedding_dim)\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    return (word, coefs)\n",
    "    \n",
    "with open('./wiki.en.vec', encoding='utf8') as f:\n",
    "#     [line[0] for line.rstrip().rsplit(' ', embedding_dim) in f]\n",
    "    embedding = dict(map(process_pretrained_word_vec, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from math import sqrt\n",
    "\n",
    "# # vect  = embedding['paris'] - embedding['france'] + embedding['italy']\n",
    "# # np.array_equal(vect, embedding['rome'])\n",
    "\n",
    "# def euclidian_distance(v1, v2):\n",
    "#     return np.sqrt(np.sum((v1 - v2) ** 2 ))\n",
    "\n",
    "# # vect = euclidian_distance(embedding['france'], embedding['paris']) + embedding['italy']\n",
    "# # nearest_vect = [(k, euclidian_distance())]\n",
    "# # [k for k, v in embedding.items() if np.array_equal(v, vect)]\n",
    "\n",
    "# def find_nearest_word(word):\n",
    "#     base_vect = embedding[word]\n",
    "#     words_with_distance = [(k, euclidian_distance(base_vect, v)) for k, v in embedding.items()]\n",
    "#     return sorted(words_with_distance, key=lambda x: x[1])[1:15]\n",
    "\n",
    "# # print(find_nearest_word('spain'))\n",
    "\n",
    "# def closest_analogy(left1, left2, right1):\n",
    "#     vec = (embedding[left1] - embedding[left2]) + embedding[right1]\n",
    "#     words_with_distance = [(k, euclidian_distance(vec, v)) for k, v in embedding.items()]\n",
    "#     return sorted(words_with_distance, key=lambda x: x[1])[0:15]\n",
    "\n",
    "# closest_analogy('paris', 'france', 'rome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.065334  , -0.093031  , -0.017571  , ...,  0.16642   ,\n",
       "        -0.13079   ,  0.035397  ],\n",
       "       [-0.21341   ,  0.15353   ,  0.05288   , ..., -0.025937  ,\n",
       "        -0.072507  ,  0.14989001],\n",
       "       ...,\n",
       "       [ 0.20563   ,  0.18877   , -0.61066997, ...,  0.43869999,\n",
       "        -0.19874001,  0.32304999],\n",
       "       [-0.25375   , -0.24808   , -0.17106   , ...,  0.28101999,\n",
       "         0.30978999,  0.233     ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 70, 300)           54959400  \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 70, 128)           192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 23, 128)           0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 55,158,796\n",
      "Trainable params: 55,158,540\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization, Dense, Dropout\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(embedding_matrix),\n",
    "                    embedding_dim, weights=[embedding_matrix],\n",
    "                    input_length=max_len, trainable=True)\n",
    "         )\n",
    "\n",
    "# Add Convolutional layer\n",
    "model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(BatchNormalization())\n",
    "# Add fully connected layers\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "127656/127656 [==============================] - 63s 490us/step - loss: 0.0667 - acc: 0.9760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d7804e080>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.957432  , 0.18879408, 0.12028662, 0.7672841 , 0.1591645 ,\n",
       "        0.01995854]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pad_sequences(tokenizer.texts_to_sequences([\"I will kill you \"]), maxlen=max_len, padding='post')\n",
    "model.predict([test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.981859497387775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred = model.predict(padded_test)\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.9822522540504516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 70, 300)           54959400  \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 70, 60)            86640     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 70, 128)           38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 23, 128)           0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 55,091,836\n",
      "Trainable params: 55,091,580\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "model_rnn = Sequential()\n",
    "\n",
    "model_rnn.add(Embedding(len(embedding_matrix),\n",
    "                    embedding_dim, weights=[embedding_matrix],\n",
    "                    input_length=max_len, trainable=True)\n",
    "         )\n",
    "\n",
    "model_rnn.add(LSTM(60, return_sequences=True, name='lstm_layer'))\n",
    "model_rnn.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "model_rnn.add(MaxPooling1D(3))\n",
    "model_rnn.add(GlobalMaxPooling1D())\n",
    "model_rnn.add(BatchNormalization())\n",
    "# Add fully connected layers\n",
    "model_rnn.add(Dense(50, activation='relu'))\n",
    "model_rnn.add(Dropout(0.3))\n",
    "model_rnn.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model_rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "127656/127656 [==============================] - 390s 3ms/step - loss: 0.0484 - acc: 0.9820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d78047c50>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.fit(padded_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9828070415809732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred = model_rnn.predict(padded_test)\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
